Title         : 《并行程序设计》知识总结
Author        : 崔利伟
Affiliation   : 计算机科学与工程学院
Email         : cui@hellolw.com
Package       : [UTF8]ctex
name-references   : 参考
name-abstract     : 摘要
name-contents     :  目录
Locale        : Chinese

[TITLE]

~ Abstract
本文是哈尔滨工业大学《并行程序设计》一课的重点总结，内容包括并行程序基本概念、设计方法论、不同编程模型的实现、通信性能分析、同步操作和并行一致性。
~

[TOC]

# 并行计算*(Parallel computing)*总论

## 并行计算机机器*(Parallel computer)*分类
对于每个并行任务，可以按照以下方法判断其所属类别：

* 并行部分是否执行同样的指令？
* 并行部分是否作用于同样的数据？

据此可以分为(单|双)指令(单|双)数据四种组合中的一种。

### `MIMD`*(Multiple instruction, multiple data)* 进一步分类
对于每个多指令多数据计算机，可以继续划分：

* 处理器间是否共享内存?
* 处理器是否通过总线连接？

来判断是多处理器系统还是多计算机系统。

## 与分布式计算*(Distributed computing)*的区别 
* 分布式计算指多台**计算机**在**地理**上分布，利用**网络通信**进行协作来完成任务。
* 并行计算指多个**处理器同时**进行协作，分别完成某一任务的子任务。这些处理器可以在一台计算机上，也可以在多台。

因此，两者互不包含，但有交集。

# 多核处理器 *(Multiprocessor)*
## 同构*(Homogeneous)*/异构*(Heterogeneous)*多核比较
根据计算内核的地位是否对等，可分为同构多核和异构多核。

## 多核*(Multi-core)*与单核、多处理器、超线程的比较
### 与单核*(Single-core)*
单核仅有一个计算核心，其上的多个线程由`CPU`进行调度，时间片不重叠。

### 与多处理器*(Multicomputers)*
多处理器指有主板上搭载多块`CPU`，不共享`cache`，通过总线连接，而多核处理器指一块单晶硅上集成多个核心。

### 与超线程*(Hyper-Threading)*
超线程为`Intel`实现的硬件级别的多线程，通过将去往不同计算单元（如整型和浮点型）的指令流同时运行来获得提升。

# Foster 方法论及算法评估

## 方法论*(Methodology)*
*我们以找出一个具有n个元素的序列最大值来说明`Foster`方法论。*

### 划分*(Partitioning)*
由于该序列有n个元素，为了尽量精细所以将其划分为n个任务，每个任务对应一个数值。

### 通讯*(Communication)*
由于每个任务不能直接访问另一个任务的数据，所以我们需要为其建立联系。在第一步中，一半任务发送自己的值，另一半接收。之后，发送方任务终止，接收方计算两者的最大值。对剩下的任务递归该过程，一半发一半接直到剩下一个任务。这个任务的计算结果为最终结果。

### 映射*(Agglomeration)*
如果n远远大于我们的处理器数p，我们可以将n个任务进行组合，每个处理器分配$\frac{n}{p}$个子任务来使得通信的开销最小。

### 聚集*(Mapping)*
我们发现在一个物理处理器上保留这$\frac{n}{p}$子任务毫无意义。所以将这些子任务聚集为一个具有$\frac{n}{p}$个数值的任务是更好的选择。

## 评估*(Evaluation)*方法 
### 延展性 *(Scale-up)*
单处理器在给定时间内完成n的工作，并行后在同样的时间内完成m的工作，延展性为$\frac{m}{n}$。

### 加速比*(SpeedUp)*及其定律 
* 加速比：$Speedup = \frac{u}{m}$ 其中u为单个处理器完成任务用时，m为并行系统完成任务用时
* Amdahl 定律： $Maximum Speedup = \frac{S + P}{S + \frac{P}{n}} = \frac{1}{S + \frac{P}{n}}$ 其中S为任务中串行部分用时，P为并行部分用时，n为处理器数, $S + P = 1$
* Gustafson 定律： $Maximum Speedup = \frac{S + (P * n)}{S + P} = n + (1 - n) *S$

两个定律的区别在于是否认为所完成的任务量应该随着处理器增多而变大。

### 效率*(Efficiency)*
$Efficiency = \frac{Speedup * 100}{n}$

其中n为处理器数目

# 并行程序*(Parallel programs)*基本概念 
## 进程*(Process)*
进程是一个活动的实体，包括程序的代码（代码段），数据（栈，堆和数据段）和状态（程序计数器和寄存器的值）。进程间不共享以上的内容。

## 线程 *(Thread)*
线程是在单个进程中的逻辑流，分别有各自独立的栈和寄存器。但同一个进程里的线程共享虚拟地址空间，因此线程间的数据可以互相访问（除了虚拟寄存器）。

## 同步机制*(Synchronization mechanism)*
### 互斥量 *(Mutex)*
当某个线程对互斥量加锁后其他线程在该线程解锁之前无法加锁，因此被阻塞，实现互斥访问。

### 临界区*(Critical section)*
只有一个线程可以进入该区域，之后想进入的线程必须等待指导前一个线程离开。

### 信号量*(Simphore)*
信号量是一个整数变量。当信号量大于0时线程可以将其减一并继续执行；如果信号量等于零则线程被阻塞直到被通知。之前使信号量减一的进程完成任务后将信号量加一并通知等待的线程中的一个。

### 条件变量 *(Condition Variables)*
当给定的条件为真时该线程试图获得锁锁；完成任务后该线程给其他被阻塞的线程发信号进行唤醒。当给定条件为假时该线程阻塞直到受到信号（即使没有受到该线程仍会不定期自动唤醒），此时重新测试条件。

## 死锁 *(Deadlock)*
两个线程分别拥有一把对方想要的锁。此时双方都在等待对方解锁，但事实上谁都不会解锁，造成程序无法继续执行。

## 阻塞 *(Block)*
某个线程由于某种原因无法获得继续执行的权限，只好一直处于阻塞状态。

## 数据竞争*(Data race)*
多个线程试图修改一个变量时会发生竞争。则常常是由于错误地假定线程调度存在依赖关系所导致的。

# `Windows` 多线程编程

## `Win32` 标准示例
``` java
#define NUMTHREADS 4
DWORD WINAPI helloFunc(LPVOID arg) {
  return 0;
}
main() {
  HANDLE hThread[NUMTHREADS];
  for (int i = 0; i < NUMTHREADS; i++)
    hThread[i] = CreateThread(NULL, 0, helloFunc, NULL, 0, NULL);
  WaitForMultipleObject(NUMTHREADS, hThread, TRUE, INFINITE);
}
```

## `Win32`下线程同步机制*(Synchronization)*

### 通过临界区*(Critial Section)*同步线程
```java
#define NUMTHREADS 4
CRITICAL_SECTION g_cs;
int g_sum = 0;
DWORD WINAPI threadFunc(LPVOID arg) {
  g_sum += 1;
  return 0;
}
main() {
  InitializeCriticalSection(&g_cs);
  // 标准示例
  DeleteCriticalSection(&g_cs);
}
```

### 通过互斥量*(Mutexes)*同步线程
```java
#define NUMTHREADS 4
int g_sum = 0;
DWORD ThreadProc() {
  HANDLE hMutex;
  hMutex = OpenMutex(MUTEX_ALL_ACCESS, FALSE, "Mutex.Test");
  g_sum += 1;
  ReleaseMutex(hMutex);
  CloseHandle(hMutex);
  return 0;
}
main() {
  HANDLE hMutex;
  hMutex = OpenMutex(MUTEX_ALL_ACCESS, FALSE, "Mutex.Test");
  hMutex = CreateMutex(NULL, FALSE, "Mutex.Test");
  // 标准示例
  CloseHandle(hMutex);
}
```

### 通过信号量*(Semaphore)* 同步线程
```java
#define NUMTHREADS 4
HANDLE hSem;
int g_sum;
DWORD WINAPI CountFives(LPVOID arg) {
  WaitForSingleObject(hSem, INFINIT);
  g_sum += 1;
  ReleaseSemaphore(hSem, 1, NULL);
}
main() {
  hSem = CreateSemphore(NULL, 1, 1, NULL);
  // 标准示例
}
```

### 通过事件*(Event)*同步线程
``` java
HANDLE evRead, evFinish;
void readThread(LPVOID param) {
  WaitForSingleObject(evRead, INFINITE);
  SetEvent(EvFinish);
}
void WriteThread(LDVOID param) {
  SetEvent(EvRead);
}
main() {
  evRead = CreateEvent(NULL, FALSE, FALSE, NULL);
  evFinish = CreateEvent(NULL, FALSE, FALSE, NULL);
  _beginthread(ReadThread, 0, NULL);
  _beginthread(WriteThread, 0, NULL);
  WaitForSingleObject(evFinish, INFINITE);
}
```

# `Linux` 多线程编程
## `Pthread`标准程序示例
```java
  void *compute(void *);
  main() {
    pthread_t p_thread;
    pthread_attr_t attr;
    pthread_attr_init(&attr);
    int hits = 0;
    pthread_create(&p_thread,, &attr, compute, (void*)&hits);
    pthread_join(p_thread, NULL);
  }
```
## `Pthread`下线程同步机制*(Synchronization)*
### 互斥量*(Mutex)*
```java
  pthread_mutex_t lock;
  lock = PTHREAD_MUTEX_INITIALIZER;
  pthread_mutex_lock(&lock);
  /* do exclusive job */
  pthread_mutex_unlock(&lock);
``` 

### 条件变量*(Condition Variables)*
* 初始化条件变量
```java
 pthread_mutex_t mut = PTHREAD_MUTEX_INITIALIZER;
 pthread_cond_t cond = PTHREAD_COND_INITIALIZER;
```
* 发出信号
```java
  pthread_mutex_lock(&mut);
  if (...) pthread_cond_signal(&cond);
  pthread_mutex_unlock(&mut);
```
* 等待信号
```java
  pthread_mutex_lock(&mut);
  while (...) pthread_cond_wait(&cond, &mut);
  /* operation */
  pthread_mutex_unlock(&mut);
```

# `OpenMP` 多线程编程

## `OpenMP`编译制导*(OpenMP directives)*
`OpenMP`使用编译制导语句，结合库和环境变量实现共享内存环境下的并行。指导语句结构如下：

```java
  #pragma omp construct [clause [clause] ...]
```

## 数据并行*(data parallelism)*
### 使用*并行区域(region)*实现
*并行区域*指的是多个线程重复执行某一区域的代码。注意：在最后一个线程执行完并行区域之前所有线程均被阻塞。可以使用`nowait`表示符关闭同步。
```java
  omp_set_num_threads(4);
  #pragma omp parallel
  {
    int ID = omp_get_thread_num();
  }
```

### 使用 `pragma omp for`实现
指导语句`#pragma omp for` 建立在 `#pragma omp parallel`创建线程的基础上，实现多个线程自动分割循环。
```java
  #pragma omp parallel
  #pragma omp for
  for (int i = 0; i < N; i++)
    c[i] = a[i] + b[i];
```

## 指令并行*(Instruction parallelism)*
`OpenMP`使用*并行区(Parallel Sections)*为线程分配任务，即指令并行而非数据并行。
```java
  #pragma omp parallel sections
  {
    #pragma omp section
    phase1();
    #pragma omp section
    phase2();
    #pragma omp section
    phase3();
  }
```

## 其他制导*(Directive)*语句
### 私有变量*(private variables)*
在并行区外的变量均为所有线程共享，可以通过在`omp parallel for`后接入`private`来声明某变量为各个线程所私有：
```java
  float x, y; int i;
  #pragma omp parallel for private(x, y)
  for (i = 0; i < N; i++)
    c[i] = x + y;
```

### 原子性*(atmoic)*
某操作为*原子的*指该操作不可分割成更小的操作，因此不会受到线程竞争的影响。
```java
  #pragma omp parallel for
  for (int i = 0; i < n; i++)
    #pragma omp atmoic
    c[i] += i;
```

# 消息传递*(Message-Passing Paradigm)*编程
## 消息传递模型*(Message-Passing Model)*
在该模型中每个处理器负责一个单独进程，因此并不共享地址空间。通信需要通过子例程进行数据包的转移。所以包的传递可以同步或异步进行。

## `MPI` 编程
`MPI`是一个提供消息传递例程的库。
### `MPI`基础操作

* 初始化和销毁:
```java
  MPI_Init(int *argc, char ***argv);
  MPI_Finalize();
```
* 获取自身所处的通信域的大小和标号:
```java
  MPI_Comm_Size(MPI_Comm comm, int *size);
  MPI_Comm_rank(MPI_Comm comm, int *rank);
```

### `MPI`数据包的收接
* 阻塞的发送和接收：
```java
  MPI_Send(void *buf, int count, MPI_Datatype datatype,\
      int dest, int tag, MPI_Comm comm);
   
  MPI_Recv(void *buf, int count, MPI_Datatype datatype, \
      int source, int tag, MPI_Comm comm, MPI_Status *status);
 ```
* 非阻塞的发送接收和测试：
```java
  MPI_Isend(void *buf, int count, MPI_Datatype datatype,\
      int dest, int tag, MPI_Comm comm, MPI_Request* request);
      
  MPI_Irecv(void *buf, int count, MPI_Datatype datatype,\
      int source, int tag, MPI_Comm comm, MPI_Request *request);
   
  MPI_test(MPI_Rquest *request, int *flag, MPI_Status *status);
```

### `MPI`的集群通信*(Collective Communication)*
`MPI`支持以下类型：

* 聚集和散发
  ```java
  MPI_Gather(void *sendbuf, int sendcount, MPI_datatype senddatatype,\
      void *recvbuf, int recvcount, MPI_Datatype recvdatatype,\
      int target, MPI_Comm comm);
      
  MPI_Scatter(void *sendbuf, int sendcount, MPI_datatype senddatatype,\
      void *recvbuf, int recvcount, MPI_Datatype recvdatatype,\
      int source, MPI_Comm comm);
  ```
* 规约
```java
  MPI_Allreduce(void *sendbuf, void *recvbuf, int count, MPI_Datatype,\
      MPI_Op op, MPI_Comm comm);
```
* 定制
```java
  MPI_Alltoall(void *sendbuf, int sendcount, MPI_datatype senddatatype,\
      void *recvbuf, int recvcount, MPI_Datatype recvdatatype,\
      MPI_Comm comm);
```

# 并行算法的通信开销*(Communication costs)*

## 开销模型*(Cost model)*
* *Startup time(t~s~)*: 算法启动时间。
* *Per-hop time(t~h~)*: 在每一跳（节点）上需要的转发/存储延迟。由路由器的延迟决定。
* *Per-word transfer time(t~w~)*: 在链路上传输每个字所需要的时间。由链路带宽决定。


## 通信模型*(Communication model)*

### 储存转发模型*(Store-and-Forward routing)*
$t_{comm} = t_s +l(mt_w+t_h)$

其中所需传递的包为m个字，需要l跳完成转发。

t~h~较小的时可简化为：

$t_{comm} = t_s + lmt_w$


### 分组路由模型*(Cutting-Through routing)*

$t_{comm} = t_s + mt_w+ lt_h$

注意由于使用了分组路由，当t~h~较小时只需要：

$t_{comm} = t_s + mt_w$

## 不同通信拓扑*(Topology)*下的开销
我们以*圆环（ring)*、*网格环(Grid-torus)*和*超立方体(Hypercube)*在分组路由的情况下进行分析。

### 点对点*(Point-to-Point)*通信
* 圆环：$t_{sum} = t_s + mt_w$
* 网格环：$t_{sum} = t_s + mt_w$
* 超立方体：$t_{sum} = t_s + mt_w$

此时三者的开销相等。

### 一对多广播*(One-to-all broadcast)*/多对一规约*(All-to-one reduction)*
* 圆环： $t_{sum} = (t_s + mt_w) \log _2 p$
* 网格环： $t_{sum} = (t_s + mt_w) \log _2 p$
* 超立方体： $t_{sum} = (t_s + mt_w) \log _2 p$

此时三者仍然相等，这里p为处理器数目。每个接收到数据包的节点继续发送，因此时间为对数形式。

### 多对多广播*(All-to-All broadcast)*/多对多规约*(All-to-all reduction)*
* 圆环：$t_{sum} = (t_s + mt_w)(p - 1)$ 为了最大限度利用带宽，每个点直接向对方发送自己新获得的数据
* 网格环： $t_{sum} = 2t_s(\sqrt {p} - 1) + mt_w(p  - 1)$ 横向进行圆环型广播，之后进行纵向广播
* 超立方体：  $t_{sum} = \sum^{\log _2 p}_{i=1}{(t_s+2^{i - 1}m{t_w})}$  (即$t_{sum} = t_s{\log _2 p} + mt_w(p - 1)$)每个点分别和前后，上下，左右的点进行数据的交换

### 散播*(Scatter)*/聚集*(Gather)*

散播指某一个节点上的p个数据包分给包含自己在内的p个节点。聚集为散播的逆过程。与多对多广播用时相同。

* 圆环：$t_{sum} = (t_s + mt_w)(p - 1)$ 
* 网格环： $t_{sum} = 2t_s(\sqrt {p} - 1) + mt_w(p  - 1)$
* 超立方体：  $t_{sum} = \sum^{\log _2 p}_{i=1}{(t_s+2^{i - 1}m{t_w})}$(即$t_{sum} = t_s{\log _2 p} + mt_w(p - 1)$)

### 多对多定制*(All-to-All Personalized)*
每个节点都需要给其余所有节点发送不同的信息，而在多对多广播中每个节点只需要发送同样的信息，因此两者不同。

* 圆环：$t_{sum} = {\sum^{p-1}_{i=1}{(t_s + mt_w(p - i))}}$(即$t_{sum} = (t_s + mpt_w/2)(p - 1)$)
* 网格环：$t_{sum} = (2t_s + mpt_w)(\sqrt{p}-1)$
* 超立方体：$t_{sum} = (t_s + mt_w)(p-1)$

# 同步*(Synchronization)*操作

## 同步物理时钟*(Synchronizing physical clocks)*
### 内同步*(Internal synchronization)*与外同步*(External synchronization)*
* 外同步：对于某个同步界$D>0$，总满足${|S(t)-C_i(t)|} < D, for i = 1, 2, ..N$.其中节点$p_i$的时钟为$C_i$，S代表`UTC`时钟，t为某个具体时间。
* 内同步：对于某个同步界$D>0$，总满足${|C_i(t)-C_j(t)|} < D, for i, j = 1, 2, ..N$。

### Cristian 算法
1. 节点向时间服务器发送请求，同时记录从发送请求到受到回复所用的时间$T_{round}$.
受到的时间信息记为T.
2. 节点的时钟设为$T + T_{round} / 2$.

该方法为`C/S`模型，外同步。

### Berkeley 算法
1. 主节点向所有节点发送自己的时间，并要求所有节点返回自己与该时间的时间差。
2. 主节点做平均后将修改量分别发送至所有节点。

该方法为内同步。

## 同步逻辑时钟*(Synchronizing logical clocks)*

### Lamport 时间戳算法
1. 对于节点i有时间戳$L_i$.随着事件的发生有$L_i = L_i + 1$.
2. 当节点$p_i$向节点$p_j$发送一个信息时也携带自己的时间戳$t = L_i$。当节点$p_j$受到消息时计算$L_j = max(L_j, t)$，之后继续应用**规则1**。

两事件有因果时可以保证其时间戳也有序，但反之不可。

### 向量时钟*(Vector clocks)*算法
1. 每个节点$p_i$维护一个向量时钟$V_i$。一开始对于每个维度$V_i[j] = 0, for j = 1, 2..., N$。
2. 在$p_i$发生了一个事件后保证$V_i[i] = V_i[i] + 1$
3. 每个节点在发送信息时都携带时间向量$t = V_i$
4. 当$p_i$受到信息及时间向量后设置$V_i[j] = max(V_i[j], t[j]), for j = 1, 2..., N$

可以保证事件有因果关系时其向量间也有序，反之亦然。

# 一致性*(Consistency)*

## 以数据为中心*(Data-Centric)*的模型
* 严格*(Strict)*一致性：任意的写操作都会立即对所有的进程可见，因此保证了绝对的时间先后关系。
* 顺序*(Sequential)*一致性: 所有的进程对某共享数据的读操作的结果都是以相同顺序出现的。
* 因果*(Casual)*一致性: 当对某共享数据的写操作存在因果关系时，所有进程的读操作要体现该因果关系。
* 先入现出*(FIFO)*一致性： 某进程对共享数据的多个写操作间的顺序会在其他进程的读操作体现。


## 以用户为中心*(Client-Centric)*的模型

* 单调读*(Monotonic Reads)*一致性： 
如果一个进程已经读取到某个数据的一个值，那么在本地或者迁移到别的地方再进行读操作的时候之后读一定会返回这个值或者更新的值。
* 单调写*(Monotonic Writes)*一致性：
如果一个进程写一个数据，那么它在本地或者迁移到别的地方再进行写操作的时候，原来的写操作要先传播到这个位置。

* 读己之所写*(Read your Writes)*一致性：
当一个进程对一个数据有写操作，那么这个数据的任何副本上都应该看到这个写操作的影响。

* 读后写*(Writes follow reads)*一致性：
在读操作后面的写操作要基于跟读操作得到的一样新或更新的值。